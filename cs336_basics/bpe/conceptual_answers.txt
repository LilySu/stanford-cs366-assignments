run scripts:
uv venv --python 3.11
source .venv/bin/activate
uv run pytest tests/test_train_bpe.py -v



print(repr(chr(0)))
'\x00'
print(chr(0)) did not return anything.
The string representation (__repr__()) shows '\x00' as unicode
The null character otherwise does not have a string representation.


>>> test_string = "this is a test" + chr(0) + "string"
>>> print(test_string)
this is a teststring

print(len(test_string))
21

When chr(0) occurs in text, it's present in the string (increases length) but invisible when printed, making it appear that text on either side is concatenated together without the character.

a. UTF-8 is smaller than UTF-16, and therefore more compact, requiring less storage space.
UTF-8 ensures universal compatibility primarily with the entire Unicode character set (all written languages and symbols worldwide) and with legacy ASCII systems.
There are no byte order marks or platform endianness issues
b. decode_utf8_bytes_to_string_wrong in that it is decoding one character at a time sequentially. 
This would fail due to failure of byte-alignment when certain characters may fail at decoding.
Instead, using .decode will process the entire bytestring at the same time to handle characters where 
the starting byte may not be a valid UTF-8 datatype. Certain characters are continuation bytes that 
will result in an error. 
c. b'\xc2\x00'

BPE Training on TinyStories:
a. Training took approximately 0.63 minutes (~37.59 seconds) and used less than 1GB of RAM (well under the 30GB limit). The longest token in the vocabulary is " congratulations" at 16 bytes, which makes sense as it's a common encouraging word in children's stories.

b. The get_pair_counts function takes the most time during tokenizer training, consuming approximately 50% of total execution time (26.8 seconds out of 53.7 seconds), followed by merge_pair at 21% of total time (11.5 seconds).




3.6
(a) Parameters and MemoryOur model would have approximately 1.55 billion trainable parameters. To load this model in single-precision floating point (4 bytes per parameter), it requires approximately 6.2 GB of memory.(Calculation: $12 \times 48 \times 1600^2 \approx 1.47B$ in layers + $80M$ in embeddings).(b) Matrix Multiplies and FLOPsFor a sequence length $T$ and hidden dimension $d$:Q, K, V Projections: 3 multiplications of $(T \times d) \times (d \times d)$. Total: $6Td^2$ FLOPs per layer.Attention Scores ($QK^T$): $h$ multiplications of $(T \times d_h) \times (d_h \times T)$. Total: $2T^2d$ FLOPs per layer.Attention Weighted Sum ($AV$): $h$ multiplications of $(T \times T) \times (T \times d_h)$. Total: $2T^2d$ FLOPs per layer.Attention Output Projection: 1 multiplication of $(T \times d) \times (d \times d)$. Total: $2Td^2$ FLOPs per layer.FFN Expansion (W1): 1 multiplication of $(T \times d) \times (d \times 4d)$. Total: $8Td^2$ FLOPs per layer.FFN Projection (W2): 1 multiplication of $(T \times 4d) \times (4d \times d)$. Total: $8Td^2$ FLOPs per layer.Logits: 1 multiplication of $(T \times d) \times (d \times V)$. Total: $2TdV$ FLOPs.Total FLOPs: $L \cdot (24Td^2 + 4T^2d) + 2TdV$.(c) Most Expensive ComponentThe Feed-Forward Networks (FFN) require the most FLOPs (specifically the large projections $d \to 4d$ and back). They consume roughly $2/3$ of the compute in the Transformer blocks compared to the $1/3$ used by the Attention projections.(d) Scaling Model SizeGPT-2 Small: The embedding/logit layer (dependent on $V$) accounts for a larger chunk of total FLOPs compared to larger models.GPT-2 Large: The internal linear layers (Attn/FFN projections, dependent on $d^2$) dominate the FLOP count.Trend: As model size ($d_{model}$) increases, the linear projections ($O(d^2)$) consume proportionally more of the total FLOPs, while the attention dot-products ($O(d)$) and vocabulary projections consume proportionally less.(e) Increasing Context LengthIncreasing the context length to 16,384 significantly increases the total FLOPs, primarily driven by the Attention mechanism. The relative contribution of the Attention Score calculation ($QK^T$) and Weighted Sum ($AV$) increases drastically because they scale quadratically with sequence length ($O(T^2)$), while the rest of the model scales linearly ($O(T)$).