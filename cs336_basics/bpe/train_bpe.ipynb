{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a668c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from typing import BinaryIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32346dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He sa\n",
      "Found 21 unique pre-tokens\n",
      "\n",
      "==================================================\n",
      "Vocabulary size: 257\n",
      "Number of merges: 0\n",
      "\n",
      "First 10 merges:\n",
      "\n",
      "Last 10 vocabulary entries:\n",
      "247: b'\\xf7'\n",
      "248: b'\\xf8'\n",
      "249: b'\\xf9'\n",
      "250: b'\\xfa'\n",
      "251: b'\\xfb'\n",
      "252: b'\\xfc'\n",
      "253: b'\\xfd'\n",
      "254: b'\\xfe'\n",
      "255: b'\\xff'\n",
      "256: b'<|endoftext|>'\n"
     ]
    }
   ],
   "source": [
    "def get_token_frequencies(text, pattern, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Takes text and a compiled regex pattern.\n",
    "    Returns a frequency table: { 'token': count, ... }\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Step 1: Split on special tokens if provided\n",
    "    if special_tokens:\n",
    "        # Build pattern: escape each special token and join with |\n",
    "        split_pattern = \"|\".join(re.escape(token) for token in special_tokens)\n",
    "        # Split the text, keeping only non-empty chunks\n",
    "        chunks = [chunk for chunk in re.split(split_pattern, text) if chunk]\n",
    "    else:\n",
    "        chunks = [text]\n",
    "    \n",
    "    # Step 2: Pre-tokenize each chunk separately\n",
    "    for chunk in chunks:\n",
    "        # iterate using finditer (memory efficient)\n",
    "        for match in pattern.finditer(chunk):\n",
    "            # Extract the actual string matched\n",
    "            token = match.group().encode(\"utf-8\")\n",
    "            \n",
    "            # Manual counting logic\n",
    "            if token in stats:\n",
    "                stats[token] += 1\n",
    "            else:\n",
    "                stats[token] = 1\n",
    "            \n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_pair_frequencies(vocab):\n",
    "    \"\"\"\n",
    "    Given a vocab dict like {('l','o','w'): 5, ('l','o','w','e','r'): 2}\n",
    "    Returns pair counts like {('l','o'): 7, ('o','w'): 7, ('w','e'): 2, ...}\n",
    "    \"\"\"\n",
    "    pairs = {}\n",
    "    \n",
    "    # Loop through each token and its frequency\n",
    "    for token_bytes, freq in vocab.items():\n",
    "        # Look at consecutive pairs in this token\n",
    "        # Example: ('l','o','w') → pairs are ('l','o') and ('o','w')\n",
    "        for i in range(len(token_bytes) - 1):\n",
    "            # Get pair at position i\n",
    "            pair = (bytes([token_bytes[i]]), bytes([token_bytes[i + 1]]))\n",
    "            \n",
    "            # Add this pair's count (weighted by token frequency)\n",
    "            if pair in pairs:\n",
    "                pairs[pair] += freq\n",
    "            else:\n",
    "                pairs[pair] = freq\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def get_best_pair(pair_frequencies):\n",
    "    \"\"\"\n",
    "    Returns the pair with highest frequency.\n",
    "    In case of tie, returns lexicographically greater pair.\n",
    "    \"\"\"\n",
    "    if not pair_frequencies:\n",
    "        return None\n",
    "    \n",
    "    # Find the maximum frequency\n",
    "    max_freq = max(pair_frequencies.values())\n",
    "    \n",
    "    # Get all pairs with that frequency\n",
    "    top_pairs = [pair for pair, freq in pair_frequencies.items() if freq == max_freq]\n",
    "    \n",
    "    # Return the lexicographically greatest one\n",
    "    # max() on tuples compares element by element\n",
    "    return max(top_pairs)\n",
    "\n",
    "def merge_pair_in_token(token_bytes, pair_to_merge):\n",
    "    \"\"\"\n",
    "    Merge a specific pair in a single token.\n",
    "    Returns the modified token as bytes.\n",
    "    \"\"\"\n",
    "    if len(token_bytes) < 2:\n",
    "        return token_bytes\n",
    "    \n",
    "    new_token = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(token_bytes):\n",
    "        # Check if we can merge at position i\n",
    "        if (i < len(token_bytes) - 1 and \n",
    "            bytes([token_bytes[i]]) == pair_to_merge[0] and \n",
    "            bytes([token_bytes[i + 1]]) == pair_to_merge[1]):\n",
    "            # Merge! Combine the two bytes\n",
    "            new_token.append(pair_to_merge[0] + pair_to_merge[1])\n",
    "            i += 2\n",
    "        else:\n",
    "            # Don't merge, just copy this byte\n",
    "            new_token.append(bytes([token_bytes[i]]))\n",
    "            i += 1\n",
    "    \n",
    "    # Concatenate all byte sequences\n",
    "    return b''.join(new_token)\n",
    "\n",
    "def merge_pair(token_frequencies, pair_to_merge):\n",
    "    \"\"\"\n",
    "    Takes vocab and a pair like ('s', 't')\n",
    "    Returns new vocab where that pair is merged everywhere\n",
    "    Example: ('n','e','w','e','s','t') → ('n','e','w','e','st') if merging ('s','t')\n",
    "    \"\"\"\n",
    "    new_freq_table = {}\n",
    "    \n",
    "    for token_bytes, freq in token_frequencies.items():\n",
    "        # We'll build a new version of this token\n",
    "        new_token = merge_pair_in_token(token_bytes, pair_to_merge)\n",
    "        \n",
    "        if new_token in token_frequencies:\n",
    "            new_freq_table[new_token] += freq\n",
    "        else:\n",
    "            new_freq_table[new_token] = freq\n",
    "    \n",
    "    return new_freq_table\n",
    "\n",
    "\n",
    "def train_bpe(input_path=\"\", vocab_size=1000, special_tokens=[\"<|endoftext|>\"]):\n",
    "\n",
    "    if special_tokens is None:\n",
    "        special_tokens = []\n",
    "    \n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    next_id = 256\n",
    "\n",
    "    for special_token in special_tokens:\n",
    "        vocab[next_id] = special_token.encode(\"utf-8\")\n",
    "        next_id += 1\n",
    "\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        raw_data = f.read(vocab_size).decode(\"utf-8\", errors=\"ignore\")\n",
    "        print(raw_data)\n",
    "\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    compiled_pattern = re.compile(PAT)\n",
    "    token_frequencies = get_token_frequencies(raw_data, compiled_pattern, special_tokens)\n",
    "    print(f\"Found {len(token_frequencies)} unique pre-tokens\")\n",
    "\n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    if num_merges <= 0:\n",
    "        return vocab, []\n",
    "    print(f\"Performing {num_merges} merges...\")\n",
    "\n",
    "    merges = []  # Track the sequence of merges\n",
    "    \n",
    "    for merge_num in range(num_merges):\n",
    "        # Step 1: Count all pairs\n",
    "        pair_freq = get_pair_frequencies(token_frequencies)\n",
    "        \n",
    "        if not pair_freq:\n",
    "            print(f\"No more pairs to merge at step {merge_num}\")\n",
    "            break\n",
    "        \n",
    "        # Step 2: Find best pair\n",
    "        best_pair = get_best_pair(pair_freq)\n",
    "        \n",
    "        # Step 3: Merge it\n",
    "        token_frequencies = merge_pair(token_frequencies, best_pair)\n",
    "        \n",
    "        # Step 4: Record this merge\n",
    "        merged_token = best_pair[0] + best_pair[1]\n",
    "        vocab[next_id] = merged_token\n",
    "        next_id += 1\n",
    "\n",
    "        merges.append(best_pair)\n",
    "        \n",
    "        if (merge_num + 1) % 100 == 0 or merge_num < 10:\n",
    "            print(f\"Merge {merge_num + 1}/{num_merges}: \"\n",
    "                  f\"'{best_pair[0]!r} {best_pair[1]!r} -> {merged_token!r} \"\n",
    "                  f\"(freq: {pair_freq[best_pair]})\")\n",
    "\n",
    "    print(f\"\\nTraining complete! Final vocab size: {len(vocab)}\")\n",
    "    return vocab, merges\n",
    "\n",
    "vocab, merges = train_bpe(\n",
    "    input_path = \"../data/TinyStoriesV2-GPT4-train.txt\", \n",
    "    vocab_size=100, \n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Number of merges: {len(merges)}\")\n",
    "print(\"\\nFirst 10 merges:\")\n",
    "for i, (byte1, byte2) in enumerate(merges[:10]):\n",
    "    merged = byte1 + byte2\n",
    "    print(f\"{i+1}. {byte1!r} + {byte2!r} -> {merged!r}\")\n",
    "\n",
    "print(\"\\nLast 10 vocabulary entries:\")\n",
    "for idx in sorted(vocab.keys())[-10:]:\n",
    "    print(f\"{idx}: {vocab[idx]!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d88ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
